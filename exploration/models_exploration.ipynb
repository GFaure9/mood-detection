{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb9fb44",
   "metadata": {},
   "source": [
    "# Exploration of CNN models for face expression classification\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, __we explore different CNN architectures to perform classification of human facial expression images__ among the 7 following options:\n",
    "\n",
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/angry/2486.jpg\">\n",
    "        <p>Angry</p>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/disgust/7835.jpg\">\n",
    "        <p>Disgust</p>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/fear/1367.jpg\">\n",
    "        <p>Fear</p>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/happy/80.jpg\">\n",
    "        <p>Happy</p>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/sad/2418.jpg\">\n",
    "        <p>Sad</p>\n",
    "    </div>\n",
    "        <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/surprise/435.jpg\">\n",
    "        <p>Surprise</p>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 50px;\">\n",
    "        <img src=\"../datasets/test/neutral/2761.jpg\">\n",
    "        <p>Neutral</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a541e",
   "metadata": {},
   "source": [
    "## 1. Defining and Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23de3ee",
   "metadata": {},
   "source": [
    "We start by importing the `Pipeline` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a440b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60593ff",
   "metadata": {},
   "source": [
    "Then we define the configuration of the models to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b787ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture and training configurations\n",
    "\n",
    "train_pth = \"../datasets/downsample_train\"\n",
    "test_pth = \"../datasets/test\"\n",
    "archi_save_paths = [\n",
    "    \"./trained_model_tests/cfg1_archi.json\",\n",
    "    \"./trained_model_tests/cfg2_archi.json\",\n",
    "    \"./trained_model_tests/cfg3_archi.json\",\n",
    "]\n",
    "weights_save_paths = [\n",
    "    \"./trained_model_tests/cfg1_weights.h5\",\n",
    "    \"./trained_model_tests/cfg2_weights.h5\",\n",
    "    \"./trained_model_tests/cfg3_weights.h5\",\n",
    "]\n",
    "\n",
    "cfg1 = {\n",
    "    \"path_to_train\": train_pth, \n",
    "    \"path_to_test\": test_pth,\n",
    "    \"batch_size\": 32,\n",
    "    \"activation_type\": \"relu\", \n",
    "    \"conv_pool_type\": \"ConvPool\",\n",
    "    \"n_conv\": 6,\n",
    "    \"n_epoch\": 3, \n",
    "    \"archi_save_path\": archi_save_paths[0],\n",
    "    \"weights_save_path\": weights_save_paths[0],\n",
    "}\n",
    "\n",
    "cfg2 = {\n",
    "    \"path_to_train\": train_pth, \n",
    "    \"path_to_test\": test_pth,\n",
    "    \"batch_size\": 32,\n",
    "    \"activation_type\": \"relu\", \n",
    "    \"conv_pool_type\": \"ConvConvPool\",\n",
    "    \"n_conv\": 8,\n",
    "    \"n_epoch\": 3,\n",
    "    \"archi_save_path\": archi_save_paths[1],\n",
    "    \"weights_save_path\": weights_save_paths[1],\n",
    "}\n",
    "\n",
    "cfg3 = {\n",
    "    \"path_to_train\": train_pth, \n",
    "    \"path_to_test\": test_pth, \n",
    "    \"batch_size\": 32,\n",
    "    \"activation_type\": \"relu\", \n",
    "    \"conv_pool_type\": \"ConvConvPool\",\n",
    "    \"n_conv\": 9,\n",
    "    \"n_epoch\": 3,\n",
    "    \"archi_save_path\": archi_save_paths[2],\n",
    "    \"weights_save_path\": weights_save_paths[2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26171a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances of Pipeline class\n",
    "\n",
    "pipe1 = Pipeline(**cfg1)\n",
    "pipe2 = Pipeline(**cfg2)\n",
    "pipe3 = Pipeline(**cfg3)\n",
    "\n",
    "pipelines = [pipe1, pipe2, pipe3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5a548",
   "metadata": {},
   "source": [
    "We can then train the chosen models by calling the `run_train()` method for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adb5bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training pipeline...\n",
      "Found 1566 images belonging to 7 classes.\n",
      "Found 391 images belonging to 7 classes.\n",
      "Dataset pre-processed\n",
      "CNN structure created\n",
      "CNN built\n",
      "Epoch 1/3\n",
      "49/49 [==============================] - 122s 2s/step - loss: 1.6523 - accuracy: 0.5140 - val_loss: 1.6301 - val_accuracy: 0.5396\n",
      "Epoch 2/3\n",
      "49/49 [==============================] - 118s 2s/step - loss: 1.5932 - accuracy: 0.5351 - val_loss: 1.5219 - val_accuracy: 0.5396\n",
      "Epoch 3/3\n",
      "49/49 [==============================] - 118s 2s/step - loss: 1.5929 - accuracy: 0.5358 - val_loss: 1.5173 - val_accuracy: 0.5396\n",
      "CNN trained\n",
      "CNN architecture saved at: ./trained_model_tests/cfg1_archi.json\n",
      "CNN weights saved at: ./trained_model_tests/cfg1_weights.h5\n",
      "\n",
      "Starting training pipeline...\n",
      "Found 1566 images belonging to 7 classes.\n",
      "Found 391 images belonging to 7 classes.\n",
      "Dataset pre-processed\n",
      "CNN structure created\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d_17. Consider increasing the input size. Received input shape [None, 1, 1, 1024] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m weights_save_paths[i]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retrain:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# If the model has already been trained and saved we simply load it\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(archi_path) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(weights_path):\n",
      "File \u001b[1;32m~\\Documents\\Pro\\Formation_Supplémentaire\\Data_Science\\mood-detection\\exploration\\..\\src\\pipeline.py:68\u001b[0m, in \u001b[0;36mPipeline.run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# ################### Build and train CNN ###################\u001b[39;00m\n\u001b[0;32m     67\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN(architecture\u001b[38;5;241m=\u001b[39mcnn_structure, data_img\u001b[38;5;241m=\u001b[39mdata_image)\n\u001b[1;32m---> 68\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m cnn\u001b[38;5;241m.\u001b[39mtrain(n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch)\n",
      "File \u001b[1;32m~\\Documents\\Pro\\Formation_Supplémentaire\\Data_Science\\mood-detection\\exploration\\..\\src\\cnn.py:85\u001b[0m, in \u001b[0;36mCNN.build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape}\n\u001b[0;32m     84\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(n_filters, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_shape))\n\u001b[1;32m---> 85\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mActivation(activation))\n\u001b[0;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py:354\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[0;32m    348\u001b[0m             input_shape[:batch_rank]\n\u001b[0;32m    349\u001b[0m             \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters]\n\u001b[0;32m    350\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :])\n\u001b[0;32m    351\u001b[0m         )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_17. Consider increasing the input size. Received input shape [None, 1, 1, 1024] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "# Training the different models if needed else load weights\n",
    "import os\n",
    "\n",
    "retrain = True\n",
    "\n",
    "for i, pipe in enumerate(pipelines):\n",
    "    archi_path = archi_save_paths[i]\n",
    "    weights_path = weights_save_paths[i]\n",
    "    \n",
    "    if retrain:\n",
    "        pipe.run_train()\n",
    "        \n",
    "    # If the model has already been trained and saved we simply load it\n",
    "    elif os.path.exists(archi_path) and os.path.exists(weights_path):\n",
    "        pipe.trained_model_from_file()\n",
    "    \n",
    "    # If not, we build and train it and then save it\n",
    "    else:\n",
    "        pipe.run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69554759",
   "metadata": {},
   "source": [
    "It is possible to visualize built models by using the `keras.models.Model.summary()` method as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ad9fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print models\n",
    "\n",
    "for i, pipe in enumerate(pipelines):\n",
    "    print(f\"\\n\\nSummary model n°{i}:\")\n",
    "    pipe.model.network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608fd90",
   "metadata": {},
   "source": [
    "## 2. Testing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942060a",
   "metadata": {},
   "source": [
    "Let's test the models that we built and trained in order to choose the best one. This can be done easily by running the `run_test()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pipe in enumerate(pipelines):\n",
    "    print(f\"\\nRunning test for model n°{i}\")\n",
    "    pipe.run_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
